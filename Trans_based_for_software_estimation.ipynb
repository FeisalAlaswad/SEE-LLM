{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy7hZiTjXULm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch pytorch-tabular scikit-learn pandas\n",
        "!pip install pytorch-tabular\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Load Dataset\n",
        "# ----------------------------------------------------\n",
        "df = pd.read_csv(\"desharnais.csv\")\n",
        "df = df.rename(columns={'PointsAjust': 'PointsAdjust'})\n",
        "\n",
        "# Keep relevant features\n",
        "features = [\n",
        "    \"TeamExp\", \"ManagerExp\", \"Length\", \"Transactions\",\n",
        "    \"Entities\", \"PointsAdjust\", \"Language\"\n",
        "]\n",
        "target = \"Effort\"\n",
        "df = df[features + [target]]\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Log-transform continuous features\n",
        "# ----------------------------------------------------\n",
        "cols_to_log = [\"Effort\", \"Length\", \"Transactions\", \"Entities\", \"PointsAdjust\"]\n",
        "df[cols_to_log] = np.log(df[cols_to_log])\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Normalize selected features\n",
        "# ----------------------------------------------------\n",
        "cols_to_norm = [\"TeamExp\", \"ManagerExp\", \"Language\"]\n",
        "df[cols_to_norm] = StandardScaler().fit_transform(df[cols_to_norm])\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Create natural-language descriptions for transformers\n",
        "# ----------------------------------------------------\n",
        "def row_to_text(row):\n",
        "    text = (\n",
        "        f\"Team experience {row['TeamExp']}, \"\n",
        "        f\"Manager experience {row['ManagerExp']}, \"\n",
        "        f\"Project length {row['Length']}, \"\n",
        "        f\"Transactions {row['Transactions']}, \"\n",
        "        f\"Entities {row['Entities']}, \"\n",
        "        f\"Adjusted function points {row['PointsAdjust']}, \"\n",
        "        f\"Language type {row['Language']}.\"\n",
        "    )\n",
        "    return text\n",
        "\n",
        "df[\"text\"] = df.apply(row_to_text, axis=1)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Define metrics\n",
        "# ----------------------------------------------------\n",
        "def compute_metrics(y_true_log, y_pred_log):\n",
        "    y_true = np.exp(y_true_log)\n",
        "    y_pred = np.exp(y_pred_log)\n",
        "\n",
        "    mse = mean_squared_error(y_true_log, y_pred_log)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
        "    r2 = r2_score(y_true_log, y_pred_log)\n",
        "\n",
        "    mre = np.abs((y_true - y_pred) / y_true)\n",
        "    mmre = np.mean(mre)\n",
        "    pred_25 = np.mean(mre <= 0.25)\n",
        "\n",
        "    return mse, rmse, mae, r2, mmre, pred_25\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# PyTorch Dataset for text-based transformers\n",
        "# ----------------------------------------------------\n",
        "class TextRegDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tok = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tok[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tok[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "from transformers import GPT2Model\n",
        "class GPT2ForRegression(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model=\"gpt2\"):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained(pretrained_model)\n",
        "        self.regressor = torch.nn.Linear(self.gpt2.config.n_embd, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
        "        pooled = last_hidden[:, 0, :]           # use first token for regression\n",
        "        return self.regressor(pooled)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Transformer training function (MSELoss)\n",
        "# ----------------------------------------------------\n",
        "def train_transformer(model, train_loader, epochs=10):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # If outputs is a tensor (like GPT-2 regression), use it directly\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                logits = outputs.view(-1)\n",
        "            else:  # for BERT/RoBERTa\n",
        "                logits = outputs.logits.view(-1)\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        #print(f\"Epoch {epoch+1}, Train Loss = {total_loss:.4f}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Prepare for 5-Fold Cross-Validation\n",
        "# ----------------------------------------------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = []\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Define models and tokenizers\n",
        "gpt2_model = GPT2ForRegression(pretrained_model=\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # GPT-2 requires pad token\n",
        "\n",
        "transformer_models = {\n",
        "    \"BERT-base\": (BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1),\n",
        "                  BertTokenizer.from_pretrained(\"bert-base-uncased\")),\n",
        "    \"RoBERTa-base\": (RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1),\n",
        "                     RobertaTokenizer.from_pretrained(\"roberta-base\")),\n",
        "    \"GPT-2\": (gpt2_model, gpt2_tokenizer)\n",
        "}\n",
        "\n",
        "\n",
        "# GPT-2 padding\n",
        "transformer_models[\"GPT-2\"][1].pad_token = transformer_models[\"GPT-2\"][1].eos_token\n",
        "#transformer_models[\"GPT-2\"][0].resize_token_embeddings(len(transformer_models[\"GPT-2\"][1]))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5-Fold CV for text-based transformers\n",
        "# ----------------------------------------------------\n",
        "for name, (model, tokenizer) in transformer_models.items():\n",
        "    print(f\"\\nRunning 5-Fold CV for {name}...\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        train_df = df.iloc[train_index]\n",
        "        test_df  = df.iloc[test_index]\n",
        "\n",
        "        train_dataset = TextRegDataset(train_df[\"text\"].tolist(), train_df[\"Effort\"].tolist(), tokenizer)\n",
        "        test_dataset  = TextRegDataset(test_df[\"text\"].tolist(),  test_df[\"Effort\"].tolist(), tokenizer)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "        test_loader  = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "        # Train model\n",
        "        train_transformer(model, train_loader, epochs=10)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        preds, trues = [], []\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                # If outputs is a tensor (like GPT-2 regression), use it directly\n",
        "                if isinstance(outputs, torch.Tensor):\n",
        "                    logits = outputs.view(-1)\n",
        "                else:  # for BERT/RoBERTa\n",
        "                    logits = outputs.logits.view(-1)\n",
        "\n",
        "                if logits.dim() == 0:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "                preds.extend(logits.cpu().numpy())\n",
        "                trues.extend(labels.cpu().numpy())\n",
        "\n",
        "        fold_metrics.append(compute_metrics(np.array(trues), np.array(preds)))\n",
        "\n",
        "    avg_metrics = np.mean(fold_metrics, axis=0)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"MSE\": avg_metrics[0],\n",
        "        \"RMSE\": avg_metrics[1],\n",
        "        \"MAE\": avg_metrics[2],\n",
        "        \"R²\": avg_metrics[3],\n",
        "        \"MMRE\": avg_metrics[4],\n",
        "        \"Pred(25)\": avg_metrics[5]\n",
        "    })\n",
        "\n",
        "# Compute average metrics\n",
        "avg_metrics = np.mean(fold_metrics, axis=0)\n",
        "\n",
        "\n",
        "# Display Results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== FINAL COMPARISON TABLE =====\\n\")\n",
        "print(results_df.sort_values(\"RMSE\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "YJMIu4-tY3Wm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7bcd9a-df97-4fdc-8753-ab031986ca6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running 5-Fold CV for RoBERTa-base...\n",
            "\n",
            "Running 5-Fold CV for GPT-2...\n",
            "\n",
            "===== FINAL COMPARISON TABLE =====\n",
            "\n",
            "          Model       MSE      RMSE       MAE        R²      MMRE  Pred(25)\n",
            "0     BERT-base  0.794309  0.838325  0.706915 -0.166370  0.669519  0.199265\n",
            "1  RoBERTa-base  0.751170  0.853889  0.671356 -0.177613  1.064406  0.245588\n",
            "2         GPT-2  1.425389  1.116220  0.953546 -0.973379  1.157639  0.147794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# -----------------------------\n",
        "# Load and preprocess COCOMO-81\n",
        "# -----------------------------\n",
        "cocomo = pd.read_csv(\"COCOMO-81.csv\")\n",
        "\n",
        "# If column names differ, adjust these names accordingly:\n",
        "# assumed columns: 'actual' (effort), 'loc', plus cost drivers like 'rely','data','cplx',... and 'dev_mode'\n",
        "# encode dev_mode categories (embedded, semidetached, organic)\n",
        "if \"dev_mode\" in cocomo.columns:\n",
        "    cocomo[\"dev_mode\"] = cocomo[\"dev_mode\"].astype(\"category\").cat.codes\n",
        "\n",
        "# Log-transform skewed columns (target 'actual' and optionally 'loc')\n",
        "cols_to_log_transform = [c for c in [\"actual\", \"loc\"] if c in cocomo.columns]\n",
        "cocomo[cols_to_log_transform] = np.log(cocomo[cols_to_log_transform])\n",
        "\n",
        "# Columns to normalize (use the same set you used before; keep only those present)\n",
        "cols_to_normalize = [\n",
        "    'rely','data','cplx','time','stor','virt','turn',\n",
        "    'acap','aexp','pcap','vexp','lexp','modp','tool','sced',\n",
        "    'dev_mode'\n",
        "]\n",
        "cols_to_normalize = [c for c in cols_to_normalize if c in cocomo.columns]\n",
        "\n",
        "# Standard scale the normalized columns\n",
        "if cols_to_normalize:\n",
        "    cocomo[cols_to_normalize] = StandardScaler().fit_transform(cocomo[cols_to_normalize])\n",
        "\n",
        "# Select final features for the text description (choose a compact set)\n",
        "# We include 'loc' (log), normalized cost drivers, and dev_mode if available.\n",
        "text_feature_cols = []\n",
        "# include 'loc' if available (already log-transformed)\n",
        "if \"loc\" in cocomo.columns:\n",
        "    text_feature_cols.append(\"loc\")\n",
        "# include a few cost drivers (use whichever exist)\n",
        "for c in ['rely','data','cplx','time','stor','acap','aexp','pcap','tool','sced','dev_mode']:\n",
        "    if c in cocomo.columns:\n",
        "        text_feature_cols.append(c)\n",
        "\n",
        "# target column (log-scale)\n",
        "target_col = \"actual\"\n",
        "if target_col not in cocomo.columns:\n",
        "    raise ValueError(f\"Target column '{target_col}' not found in COCOMO file.\")\n",
        "\n",
        "# Keep only needed columns, drop rows with NaNs\n",
        "keep_cols = text_feature_cols + [target_col]\n",
        "cocomo = cocomo[keep_cols].dropna().reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Create natural-language text for each row\n",
        "# -----------------------------\n",
        "def row_to_text(row):\n",
        "    parts = []\n",
        "    for col in text_feature_cols:\n",
        "        val = row[col]\n",
        "        # format numeric nicely\n",
        "        parts.append(f\"{col} {float(np.round(val, 4))}\")\n",
        "    return \", \".join(parts) + \".\"\n",
        "\n",
        "cocomo[\"text\"] = cocomo.apply(row_to_text, axis=1)\n",
        "# target (log-scale)\n",
        "cocomo[\"target_log\"] = cocomo[target_col]\n",
        "\n",
        "# -----------------------------\n",
        "# Metrics (consistent with your COCOMO script)\n",
        "# Inputs to compute_metrics are log-scaled true and predicted values\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true_log, y_pred_log):\n",
        "    # numeric arrays\n",
        "    y_true_log = np.asarray(y_true_log).reshape(-1)\n",
        "    y_pred_log = np.asarray(y_pred_log).reshape(-1)\n",
        "\n",
        "    # log-scale metrics\n",
        "    mse = mean_squared_error(y_true_log, y_pred_log)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
        "    r2 = r2_score(y_true_log, y_pred_log)\n",
        "\n",
        "    # convert to real scale for MMRE and Pred(25)\n",
        "    y_true_real = np.exp(y_true_log)\n",
        "    y_pred_real = np.exp(y_pred_log)\n",
        "    mre = np.abs((y_true_real - y_pred_real) / y_true_real)\n",
        "    mmre = np.mean(mre)\n",
        "    pred_25 = np.mean(mre <= 0.25)\n",
        "\n",
        "    return mse, rmse, mae, r2, mmre, pred_25\n",
        "\n",
        "# -----------------------------\n",
        "# PyTorch Dataset for transformers\n",
        "# -----------------------------\n",
        "class TextRegDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tok = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {\n",
        "            \"input_ids\": tok[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "        return item\n",
        "\n",
        "# -----------------------------\n",
        "# GPT-2 regression wrapper (simple)\n",
        "# -----------------------------\n",
        "from transformers import GPT2Model\n",
        "class GPT2ForRegression(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model=\"gpt2\"):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained(pretrained_model)\n",
        "        hidden_size = self.gpt2.config.n_embd\n",
        "        self.regressor = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # use first token ([0]) pooling similar to your previous code\n",
        "        last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden]\n",
        "        pooled = last_hidden[:, 0, :]\n",
        "        return self.regressor(pooled).view(-1)  # return 1-d tensor per batch element\n",
        "\n",
        "# -----------------------------\n",
        "# Training function for any transformer-like model\n",
        "# -----------------------------\n",
        "def train_transformer(model, train_loader, epochs=10, lr=2e-5, print_every=10):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # outputs may be:\n",
        "            # - a 1-D tensor (GPT2ForRegression returns shape [batch])\n",
        "            # - a SequenceClassifierOutput with .logits shape [batch, 1]\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                preds = outputs.view(-1)\n",
        "            else:\n",
        "                # e.g., transformers' ForSequenceClassification\n",
        "                preds = outputs.logits.view(-1)\n",
        "\n",
        "            loss = loss_fn(preds, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # print epoch summary\n",
        "        #print(f\"Epoch {epoch+1}/{epochs} - Train loss: {total_loss:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5-Fold CV across transformer models\n",
        "# -----------------------------\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize models & tokenizers\n",
        "# NOTE: set num_labels=1 for regression using sequence classification heads\n",
        "models_and_tokenizers = {}\n",
        "\n",
        "# BERT\n",
        "bert_tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "models_and_tokenizers[\"BERT-base\"] = (bert_model, bert_tok)\n",
        "\n",
        "# RoBERTa\n",
        "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "models_and_tokenizers[\"RoBERTa-base\"] = (roberta_model, roberta_tok)\n",
        "\n",
        "# GPT-2 custom regression\n",
        "gpt2_tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT2 tokenizer needs pad token defined\n",
        "if gpt2_tok.pad_token is None:\n",
        "    gpt2_tok.pad_token = gpt2_tok.eos_token\n",
        "gpt2_model = GPT2ForRegression(pretrained_model=\"gpt2\")\n",
        "# Optionally resize embeddings if you've added pad token (not necessary when pad token equals eos_token)\n",
        "models_and_tokenizers[\"GPT-2\"] = (gpt2_model, gpt2_tok)\n",
        "\n",
        "# -----------------------------\n",
        "# 5-Fold cross validation loop\n",
        "# -----------------------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = []\n",
        "\n",
        "for name, (model, tokenizer) in models_and_tokenizers.items():\n",
        "    print(f\"\\n=== Running 5-Fold CV for {name} ===\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(cocomo)):\n",
        "        train_df = cocomo.iloc[train_idx].reset_index(drop=True)\n",
        "        test_df  = cocomo.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "        train_dataset = TextRegDataset(train_df[\"text\"].tolist(), train_df[\"target_log\"].tolist(), tokenizer)\n",
        "        test_dataset  = TextRegDataset(test_df[\"text\"].tolist(),  test_df[\"target_log\"].tolist(), tokenizer)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "        test_loader  = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "        # Train\n",
        "        train_transformer(model, train_loader, epochs=10, lr=2e-5)\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        preds_log, trues_log = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                if isinstance(outputs, torch.Tensor):\n",
        "                    logits = outputs.view(-1)\n",
        "                else:\n",
        "                    logits = outputs.logits.view(-1)\n",
        "\n",
        "                preds_log.extend(logits.cpu().numpy())\n",
        "                trues_log.extend(labels.cpu().numpy())\n",
        "\n",
        "        fold_metrics.append(compute_metrics(np.array(trues_log), np.array(preds_log)))\n",
        "        print(f\"Fold {fold+1} done for {name}.\")\n",
        "\n",
        "    # Average across folds\n",
        "    avg_metrics = np.mean(fold_metrics, axis=0)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"MSE\": avg_metrics[0],\n",
        "        \"RMSE\": avg_metrics[1],\n",
        "        \"MAE\": avg_metrics[2],\n",
        "        \"R²\": avg_metrics[3],\n",
        "        \"MMRE\": avg_metrics[4],\n",
        "        \"Pred(25)\": avg_metrics[5]\n",
        "    })\n",
        "\n",
        "# Present results\n",
        "results_df = pd.DataFrame(results).sort_values(\"RMSE\")\n",
        "print(\"\\n===== FINAL COMPARISON TABLE =====\\n\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "3siaKLa5r-B4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb91341-1851-4235-b386-a189d1ee6021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running 5-Fold CV for BERT-base ===\n",
            "Fold 1 done for BERT-base.\n",
            "Fold 2 done for BERT-base.\n",
            "Fold 3 done for BERT-base.\n",
            "Fold 4 done for BERT-base.\n",
            "Fold 5 done for BERT-base.\n",
            "\n",
            "=== Running 5-Fold CV for RoBERTa-base ===\n",
            "Fold 1 done for RoBERTa-base.\n",
            "Fold 2 done for RoBERTa-base.\n",
            "Fold 3 done for RoBERTa-base.\n",
            "Fold 4 done for RoBERTa-base.\n",
            "Fold 5 done for RoBERTa-base.\n",
            "\n",
            "=== Running 5-Fold CV for GPT-2 ===\n",
            "Fold 1 done for GPT-2.\n",
            "Fold 2 done for GPT-2.\n",
            "Fold 3 done for GPT-2.\n",
            "Fold 4 done for GPT-2.\n",
            "Fold 5 done for GPT-2.\n",
            "\n",
            "===== FINAL COMPARISON TABLE =====\n",
            "\n",
            "          Model       MSE      RMSE       MAE        R²      MMRE  Pred(25)\n",
            "1  RoBERTa-base  1.421528  1.059809  0.848401  0.522409  1.815898  0.192308\n",
            "0     BERT-base  1.505543  1.145503  0.899501  0.483921  1.582184  0.207692\n",
            "2         GPT-2  3.565083  1.856593  1.531490 -0.100161  3.879213  0.108974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nasa_transformers_cv.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# -------------------------\n",
        "# Load NASA93 dataset\n",
        "# -------------------------\n",
        "nasa = pd.read_csv(\"NASA_93_Sheet.csv\")\n",
        "\n",
        "# Selected numeric predictors (same as your ML code)\n",
        "selected_features = [\n",
        "    \"prec\",\"flex\",\"resl\",\"team\",\"pmat\",\"rely\",\"data\",\"cplx\",\"ruse\",\"docu\",\n",
        "    \"time\",\"stor\",\"pvol\",\"acap\",\"pcap\",\"pcon\",\"apex\",\"plex\",\"ltex\",\"tool\",\n",
        "    \"site\",\"sced\",\"kloc\"\n",
        "]\n",
        "\n",
        "missing = [f for f in selected_features if f not in nasa.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
        "\n",
        "# Target\n",
        "target_col = \"effort\"\n",
        "if target_col not in nasa.columns:\n",
        "    raise ValueError(f\"Target column '{target_col}' not found in CSV\")\n",
        "\n",
        "# Keep only the chosen columns + target\n",
        "df = nasa[selected_features + [target_col]].copy()\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessing\n",
        "# -------------------------\n",
        "# log-transform the target (works well for effort distributions)\n",
        "df[\"effort_log\"] = np.log(df[target_col].values)\n",
        "\n",
        "# Scale predictors (fit once on full dataset here prior to CV)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[selected_features].values)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features, index=df.index)\n",
        "\n",
        "# Merge scaled predictors and log-target into df used later\n",
        "df_scaled = pd.concat([X_scaled_df, df[[\"effort_log\"]]], axis=1)\n",
        "\n",
        "# -------------------------\n",
        "# Create natural-language descriptions from numeric features\n",
        "# -------------------------\n",
        "# We'll round scaled values to 3 decimals to keep text length reasonable.\n",
        "def row_to_text(row):\n",
        "    parts = []\n",
        "    for feat in selected_features:\n",
        "        parts.append(f\"{feat} {row[feat]:.3f}\")\n",
        "    return \", \".join(parts) + \".\"\n",
        "\n",
        "df_scaled[\"text\"] = df_scaled.apply(row_to_text, axis=1)\n",
        "df_scaled[\"label\"] = df_scaled[\"effort_log\"].values  # label in log-space\n",
        "\n",
        "# -------------------------\n",
        "# Metrics helper (same logic as your ML code)\n",
        "# Inputs: y_true_log, y_pred_log (both in log-space)\n",
        "# -------------------------\n",
        "def compute_metrics(y_true_log, y_pred_log):\n",
        "    # Convert to real space for MMRE/Pred(25)\n",
        "    y_true_real = np.exp(y_true_log)\n",
        "    y_pred_real = np.exp(y_pred_log)\n",
        "\n",
        "    mse = mean_squared_error(y_true_log, y_pred_log)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
        "    r2 = r2_score(y_true_log, y_pred_log)\n",
        "\n",
        "    mre = np.abs((y_true_real - y_pred_real) / y_true_real)\n",
        "    mmre = np.mean(mre)\n",
        "    pred_25 = np.mean(mre <= 0.25)\n",
        "\n",
        "    return mse, rmse, mae, r2, mmre, pred_25\n",
        "\n",
        "# -------------------------\n",
        "# PyTorch Dataset for text-based transformers\n",
        "# -------------------------\n",
        "class TextRegDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tok = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tok[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# -------------------------\n",
        "# Small GPT-2 regression head (like your original)\n",
        "# -------------------------\n",
        "from transformers import GPT2Model\n",
        "class GPT2ForRegression(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model=\"gpt2\"):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained(pretrained_model)\n",
        "        self.regressor = torch.nn.Linear(self.gpt2.config.n_embd, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
        "        pooled = last_hidden[:, 0, :]           # use first token for regression\n",
        "        return self.regressor(pooled).view(-1)  # return vector [batch]\n",
        "\n",
        "# -------------------------\n",
        "# Training loop (handles both HF sequence-classification models and our GPT2ForRegression)\n",
        "# -------------------------\n",
        "def train_transformer(model, train_loader, device, epochs=10, lr=2e-5, print_every=1):\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # our GPT2ForRegression returns a tensor [batch]; HF SeqClass returns object with .logits\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                preds = outputs.view(-1)\n",
        "            else:\n",
        "                preds = outputs.logits.view(-1)\n",
        "\n",
        "            loss = loss_fn(preds, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % print_every == 0:\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            #print(f\"Epoch {epoch+1}/{epochs} — train loss: {avg_loss:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 5-Fold CV setup and model definitions\n",
        "# -------------------------\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# instantiate GPT-2 custom model + tokenizer\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # set pad token\n",
        "gpt2_model = GPT2ForRegression(pretrained_model=\"gpt2\")\n",
        "# NOTE: if you add pad_token you'll often want to resize embeddings — not necessary for our custom GPT2Model head,\n",
        "# but if you use GPT2LMHeadModel you should call resize_token_embeddings.\n",
        "\n",
        "# BERT & RoBERTa models for regression using seq-classification with num_labels=1\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "\n",
        "transformer_models = {\n",
        "    \"BERT-base\": (bert_model, bert_tokenizer),\n",
        "    \"RoBERTa-base\": (roberta_model, roberta_tokenizer),\n",
        "    \"GPT-2\": (gpt2_model, gpt2_tokenizer)\n",
        "}\n",
        "\n",
        "# ensure GPT-2 tokenizer has pad token set (again)\n",
        "transformer_models[\"GPT-2\"][1].pad_token = transformer_models[\"GPT-2\"][1].eos_token\n",
        "\n",
        "# -------------------------\n",
        "# 5-Fold Cross-Validation (text-based)\n",
        "# -------------------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, (model, tokenizer) in transformer_models.items():\n",
        "    print(f\"\\n=== Running 5-Fold CV for {name} ===\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(df_scaled), 1):\n",
        "        print(f\"Fold {fold} / 5\")\n",
        "\n",
        "        train_texts = df_scaled.iloc[train_idx][\"text\"].tolist()\n",
        "        train_labels = df_scaled.iloc[train_idx][\"label\"].tolist()\n",
        "        test_texts  = df_scaled.iloc[test_idx][\"text\"].tolist()\n",
        "        test_labels = df_scaled.iloc[test_idx][\"label\"].tolist()\n",
        "\n",
        "        train_ds = TextRegDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
        "        test_ds  = TextRegDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "        test_loader  = DataLoader(test_ds, batch_size=8)\n",
        "\n",
        "        # Reset model weights per fold if you want fresh initialization.\n",
        "        # For HF models we re-load pretrained to reset classifier head; for GPT2 custom we re-init from pretrained.\n",
        "        # This keeps each fold independent.\n",
        "        if name == \"BERT-base\":\n",
        "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "            tokenizer = bert_tokenizer\n",
        "        elif name == \"RoBERTa-base\":\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "            tokenizer = roberta_tokenizer\n",
        "        else:  # GPT-2\n",
        "            model = GPT2ForRegression(pretrained_model=\"gpt2\")\n",
        "            tokenizer = gpt2_tokenizer\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Train\n",
        "        train_transformer(model, train_loader, device=device, epochs=10, lr=2e-5)\n",
        "\n",
        "        # Evaluate\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        trues = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                if isinstance(outputs, torch.Tensor):\n",
        "                    logits = outputs.view(-1)\n",
        "                else:\n",
        "                    logits = outputs.logits.view(-1)\n",
        "\n",
        "                # ensure shapes\n",
        "                if logits.dim() == 0:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "\n",
        "                preds.extend(logits.cpu().numpy())\n",
        "                trues.extend(labels.cpu().numpy())\n",
        "\n",
        "        fold_metrics.append(compute_metrics(np.array(trues), np.array(preds)))\n",
        "\n",
        "    # average across folds\n",
        "    avg_metrics = np.mean(fold_metrics, axis=0)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"MSE\": avg_metrics[0],\n",
        "        \"RMSE\": avg_metrics[1],\n",
        "        \"MAE\": avg_metrics[2],\n",
        "        \"R²\": avg_metrics[3],\n",
        "        \"MMRE\": avg_metrics[4],\n",
        "        \"Pred(25)\": avg_metrics[5]\n",
        "    })\n",
        "\n",
        "# Final results DataFrame\n",
        "results_df = pd.DataFrame(results).sort_values(\"RMSE\").reset_index(drop=True)\n",
        "print(\"\\n===== FINAL 5-FOLD CV RESULTS =====\\n\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E13z3TlzuE6A",
        "outputId": "730b8a9f-6268-4a98-ccbb-93c07929db43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 / 5\n",
            "Fold 3 / 5\n",
            "Fold 4 / 5\n",
            "Fold 5 / 5\n",
            "\n",
            "===== FINAL 5-FOLD CV RESULTS =====\n",
            "\n",
            "          Model       MSE      RMSE       MAE        R²      MMRE  Pred(25)\n",
            "0  RoBERTa-base  2.277996  1.485058  1.242570 -0.063013  2.915608  0.107018\n",
            "1     BERT-base  4.266690  2.025075  1.697660 -1.034724  0.936477  0.128070\n",
            "2         GPT-2  4.765156  2.132437  1.808653 -1.316325  0.994240  0.074854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# maxwell_transformer_cv.py\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ---------------------------\n",
        "# Load Maxwell dataset\n",
        "# ---------------------------\n",
        "projects = pd.read_csv(\"maxwell.csv\")\n",
        "\n",
        "# Drop Syear if present (same as your ML code)\n",
        "if 'Syear' in projects.columns:\n",
        "    projects.drop(['Syear'], axis=1, inplace=True)\n",
        "\n",
        "target = 'Effort'\n",
        "feature_cols = [c for c in projects.columns if c != target]\n",
        "\n",
        "# ---------------------------\n",
        "# Log-transform skewed continuous variables (if present)\n",
        "# ---------------------------\n",
        "cols_to_log = ['Effort', 'Duration', 'Size', 'Time']\n",
        "for col in cols_to_log:\n",
        "    if col in projects.columns:\n",
        "        # replace 0 with 1 to avoid -inf, then log\n",
        "        projects[col] = np.log(projects[col].replace(0, 1).astype(float))\n",
        "\n",
        "# ---------------------------\n",
        "# Normalize categorical-like integer columns\n",
        "# ---------------------------\n",
        "categorical_like = [\n",
        "    'App','Har','Dba','Ifc','Source','Telonuse','Nlan'\n",
        "] + [f\"T{i:02d}\" for i in range(1,16) if f\"T{i:02d}\" in projects.columns]\n",
        "\n",
        "# Only scale those that actually exist\n",
        "categorical_like = [c for c in categorical_like if c in projects.columns]\n",
        "if categorical_like:\n",
        "    projects[categorical_like] = StandardScaler().fit_transform(projects[categorical_like])\n",
        "\n",
        "# Final X, y (y is in log-space already if Effort existed and was transformed)\n",
        "if target not in projects.columns:\n",
        "    raise ValueError(f\"Target column '{target}' not found in the dataset.\")\n",
        "\n",
        "X = projects.drop(columns=[target])\n",
        "y = projects[target].values  # will be log(Effort) if Effort was in cols_to_log\n",
        "\n",
        "# ---------------------------\n",
        "# Create natural-language descriptions automatically\n",
        "# ---------------------------\n",
        "def row_to_text_auto(row, feature_names):\n",
        "    parts = []\n",
        "    for f in feature_names:\n",
        "        # make short readable component\n",
        "        parts.append(f\"{f} {row[f]}\")\n",
        "    return \". \".join(parts) + \".\"\n",
        "\n",
        "projects['text'] = projects.apply(lambda r: row_to_text_auto(r, X.columns), axis=1)\n",
        "\n",
        "# ---------------------------\n",
        "# Metrics (same structure as original: metrics computed in log-space,\n",
        "# MMRE/Pred(25) computed in real space after exp)\n",
        "# ---------------------------\n",
        "def compute_metrics(y_true_log, y_pred_log):\n",
        "    \"\"\"\n",
        "    Input: arrays in log-space (i.e., log(Effort))\n",
        "    Returns: mse, rmse, mae, r2 (all on log-space), mmre, pred25 (on real scale)\n",
        "    \"\"\"\n",
        "    # ensure numpy arrays\n",
        "    y_true_log = np.array(y_true_log).astype(float)\n",
        "    y_pred_log = np.array(y_pred_log).astype(float)\n",
        "\n",
        "    # metrics on log-space\n",
        "    mse = mean_squared_error(y_true_log, y_pred_log)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
        "    r2 = r2_score(y_true_log, y_pred_log)\n",
        "\n",
        "    # convert to real space for relative error metrics\n",
        "    y_true = np.exp(y_true_log)\n",
        "    y_pred = np.exp(y_pred_log)\n",
        "    # avoid division by zero (mask where y_true==0)\n",
        "    eps = 1e-9\n",
        "    mre = np.abs((y_true - y_pred) / (y_true + eps))\n",
        "    mmre = np.mean(mre)\n",
        "    pred_25 = np.mean(mre <= 0.25)\n",
        "\n",
        "    return mse, rmse, mae, r2, mmre, pred_25\n",
        "\n",
        "# ---------------------------\n",
        "# PyTorch Dataset for text\n",
        "# ---------------------------\n",
        "class TextRegDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tok = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tok[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# ---------------------------\n",
        "# GPT-2 regression wrapper\n",
        "# ---------------------------\n",
        "from transformers import GPT2Model\n",
        "class GPT2ForRegression(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model=\"gpt2\"):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained(pretrained_model)\n",
        "        self.regressor = torch.nn.Linear(self.gpt2.config.n_embd, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
        "        pooled = last_hidden[:, 0, :]           # use first token for regression\n",
        "        return self.regressor(pooled)           # [batch, 1]\n",
        "\n",
        "# ---------------------------\n",
        "# Transformer training function (MSELoss)\n",
        "# ---------------------------\n",
        "def train_transformer(model, train_loader, epochs=10, lr=2e-5, device=None):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # handle outputs shape:\n",
        "            # - GPT2ForRegression returns [batch, 1] tensor\n",
        "            # - HuggingFace SequenceClassification models return ModelOutput with .logits [batch, 1]\n",
        "            if isinstance(outputs, torch.Tensor):\n",
        "                logits = outputs.view(-1)\n",
        "            else:\n",
        "                # e.g., transformers' ModelOutput\n",
        "                logits = outputs.logits.view(-1)\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "        avg_loss = total_loss / (len(train_loader.dataset) + 1e-9)\n",
        "        #print(f\"Epoch {epoch+1}/{epochs} — Train Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare 5-Fold CV\n",
        "# ---------------------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "results = []\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# instantiate models/tokenizers\n",
        "print(\"Loading models and tokenizers (this may take a while)...\")\n",
        "# BERT and RoBERTa as regression by using num_labels=1\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# GPT-2 custom regression\n",
        "gpt2_model = GPT2ForRegression(pretrained_model=\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# ensure pad token exists for GPT-2\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "transformer_models = {\n",
        "    \"BERT-base\": (bert_model, bert_tokenizer),\n",
        "    \"RoBERTa-base\": (roberta_model, roberta_tokenizer),\n",
        "    \"GPT-2\": (gpt2_model, gpt2_tokenizer)\n",
        "}\n",
        "# For GPT-2 ensure model's embeddings match tokenizer (optional)\n",
        "# gpt2_model.gpt2.resize_token_embeddings(len(gpt2_tokenizer))\n",
        "\n",
        "# ---------------------------\n",
        "# 5-Fold CV loop\n",
        "# ---------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "for name, (model, tokenizer) in transformer_models.items():\n",
        "    print(f\"\\n=== Running 5-Fold CV for {name} ===\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(projects), 1):\n",
        "        print(f\"Fold {fold} — preparing data\")\n",
        "        train_df = projects.iloc[train_idx].reset_index(drop=True)\n",
        "        test_df = projects.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "        train_texts = train_df['text'].tolist()\n",
        "        train_labels = train_df[target].tolist()  # log-space labels\n",
        "        test_texts = test_df['text'].tolist()\n",
        "        test_labels = test_df[target].tolist()\n",
        "\n",
        "        train_dataset = TextRegDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
        "        test_dataset  = TextRegDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "        test_loader  = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "        # train (note: models are reused across folds, so re-load fresh weights to avoid leakage)\n",
        "        # Reload model weights from pretrained each fold for fair CV\n",
        "        if name == \"BERT-base\":\n",
        "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        elif name == \"RoBERTa-base\":\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "            tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "        else:  # GPT-2\n",
        "            model = GPT2ForRegression(pretrained_model=\"gpt2\")\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        train_transformer(model, train_loader, epochs=10, lr=2e-5, device=device)\n",
        "\n",
        "        # evaluation\n",
        "        model.eval()\n",
        "        preds, trues = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                if isinstance(outputs, torch.Tensor):\n",
        "                    logits = outputs.view(-1)\n",
        "                else:\n",
        "                    logits = outputs.logits.view(-1)\n",
        "\n",
        "                # handle zero-dim tensors\n",
        "                if logits.dim() == 0:\n",
        "                    logits = logits.unsqueeze(0)\n",
        "\n",
        "                preds.extend(logits.cpu().numpy().tolist())\n",
        "                trues.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "        # compute metrics for this fold (inputs are log-space)\n",
        "        fold_metric = compute_metrics(np.array(trues), np.array(preds))\n",
        "        print(f\"Fold {fold} metrics (MSE,RMSE,MAE,R2,MMRE,Pred25): {np.round(fold_metric,4)}\")\n",
        "        fold_metrics.append(fold_metric)\n",
        "\n",
        "    # average across folds\n",
        "    avg_metrics = np.mean(fold_metrics, axis=0)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"MSE\": avg_metrics[0],\n",
        "        \"RMSE\": avg_metrics[1],\n",
        "        \"MAE\": avg_metrics[2],\n",
        "        \"R²\": avg_metrics[3],\n",
        "        \"MMRE\": avg_metrics[4],\n",
        "        \"Pred(25)\": avg_metrics[5]\n",
        "    })\n",
        "\n",
        "# results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== 5-FOLD TRANSFORMER CV RESULTS =====\\n\")\n",
        "print(results_df.sort_values(\"RMSE\").reset_index(drop=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og_HGF9avkHF",
        "outputId": "3b7114f3-998c-48d7-c77f-5cb558ae491b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 metrics (MSE,RMSE,MAE,R2,MMRE,Pred25): [ 5.4917  2.3434  2.1397 -5.0115  0.8122  0.    ]\n",
            "Fold 5 — preparing data\n",
            "Fold 5 metrics (MSE,RMSE,MAE,R2,MMRE,Pred25): [ 15.5607   3.9447   3.8269 -16.0006   0.9662   0.    ]\n",
            "\n",
            "===== 5-FOLD TRANSFORMER CV RESULTS =====\n",
            "\n",
            "          Model        MSE      RMSE       MAE         R²      MMRE  Pred(25)\n",
            "0  RoBERTa-base   4.350357  2.054900  1.816770  -3.595136  0.736251  0.114103\n",
            "1         GPT-2  13.240717  3.453824  3.301733 -13.662509  0.905722  0.000000\n",
            "2     BERT-base  16.049604  3.971133  3.851986 -16.173669  0.963143  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# ====================================================\n",
        "# Load Kitchenham Dataset\n",
        "# ====================================================\n",
        "df = pd.read_csv(\"kitchenham.csv\")\n",
        "\n",
        "# Keep only the useful features (same as your ML code)\n",
        "keep_cols = [\n",
        "    \"Project.type\",\n",
        "    \"Actual.duration\",\n",
        "    \"Actual.effort\",\n",
        "    \"Adjusted.function.points\",\n",
        "    \"First.estimate\"\n",
        "]\n",
        "df = df[keep_cols].copy()\n",
        "\n",
        "target = \"Actual.effort\"\n",
        "\n",
        "# ====================================================\n",
        "# Label encode categorical columns\n",
        "# ====================================================\n",
        "cat_cols = df.select_dtypes(include=\"object\").columns\n",
        "label_encoders = {}\n",
        "\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# ====================================================\n",
        "# Log-transform skewed numeric columns\n",
        "# (same as previous code)\n",
        "# ====================================================\n",
        "cols_to_log = [\"Actual.effort\", \"Actual.duration\",\n",
        "               \"Adjusted.function.points\", \"First.estimate\"]\n",
        "\n",
        "for col in cols_to_log:\n",
        "    df[col] = np.log(df[col].replace(0, 1))\n",
        "\n",
        "# ====================================================\n",
        "# Scale numeric features\n",
        "# ====================================================\n",
        "num_cols = [c for c in df.columns if c != target]\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# ====================================================\n",
        "# Create natural language description for each row\n",
        "# ====================================================\n",
        "def row_to_text(row):\n",
        "    text = (\n",
        "        f\"Project type encoded as {row['Project.type']}. \"\n",
        "        f\"Project duration {row['Actual.duration']}. \"\n",
        "        f\"Adjusted function points {row['Adjusted.function.points']}. \"\n",
        "        f\"First estimation value {row['First.estimate']}.\"\n",
        "    )\n",
        "    return text\n",
        "\n",
        "df[\"text\"] = df.apply(row_to_text, axis=1)\n",
        "\n",
        "# ====================================================\n",
        "# Metrics\n",
        "# ====================================================\n",
        "def compute_metrics(y_true_log, y_pred_log):\n",
        "    y_true = np.exp(y_true_log)\n",
        "    y_pred = np.exp(y_pred_log)\n",
        "\n",
        "    mse = mean_squared_error(y_true_log, y_pred_log)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true_log, y_pred_log)\n",
        "    r2 = r2_score(y_true_log, y_pred_log)\n",
        "\n",
        "    mre = np.abs((y_true - y_pred) / y_true)\n",
        "    mmre = np.mean(mre)\n",
        "    pred_25 = np.mean(mre <= 0.25)\n",
        "\n",
        "    return mse, rmse, mae, r2, mmre, pred_25\n",
        "\n",
        "# ====================================================\n",
        "# PyTorch Dataset\n",
        "# ====================================================\n",
        "class TextRegDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tok = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tok[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tok[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# ====================================================\n",
        "# GPT-2 Regression Head\n",
        "# ====================================================\n",
        "from transformers import GPT2Model\n",
        "\n",
        "class GPT2ForRegression(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model=\"gpt2\"):\n",
        "        super().__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained(pretrained_model)\n",
        "        self.regressor = torch.nn.Linear(self.gpt2.config.n_embd, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state\n",
        "        pooled = last_hidden[:, 0, :]\n",
        "        return self.regressor(pooled)\n",
        "\n",
        "# ====================================================\n",
        "# Transformer training loop\n",
        "# ====================================================\n",
        "def train_transformer(model, train_loader, epochs=10):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # GPT-2 returns a tensor; BERT/RoBERTa return SequenceClassifierOutput\n",
        "            if hasattr(outputs, \"logits\"):\n",
        "                logits = outputs.logits.squeeze(-1)    # [batch]\n",
        "            else:\n",
        "                logits = outputs.squeeze(-1)           # [batch]\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# ====================================================\n",
        "# Prepare 5-fold CV\n",
        "# ====================================================\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = []\n",
        "\n",
        "# Transformer models\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "\n",
        "transformer_models = {\n",
        "    \"BERT-base\": (\n",
        "        BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1),\n",
        "        BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    ),\n",
        "    \"RoBERTa-base\": (\n",
        "        RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1),\n",
        "        RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "    ),\n",
        "    \"GPT-2\": (\n",
        "        GPT2ForRegression(\"gpt2\"),\n",
        "        GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    )\n",
        "}\n",
        "\n",
        "# GPT-2 padding\n",
        "transformer_models[\"GPT-2\"][1].pad_token = transformer_models[\"GPT-2\"][1].eos_token\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# Run 5-fold CV\n",
        "# ====================================================\n",
        "for name, (model, tokenizer) in transformer_models.items():\n",
        "    print(f\"\\nRunning 5-Fold CV for {name}...\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    for train_idx, test_idx in kf.split(df):\n",
        "        train_df = df.iloc[train_idx]\n",
        "        test_df = df.iloc[test_idx]\n",
        "\n",
        "        train_ds = TextRegDataset(train_df[\"text\"].tolist(), train_df[target].tolist(), tokenizer)\n",
        "        test_ds = TextRegDataset(test_df[\"text\"].tolist(), test_df[target].tolist(), tokenizer)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=8)\n",
        "\n",
        "        train_transformer(model, train_loader, epochs=10)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        preds, trues = [], []\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "                # GPT-2 returns a tensor; BERT/RoBERTa return SequenceClassifierOutput\n",
        "                if hasattr(outputs, \"logits\"):\n",
        "                    logits = outputs.logits.squeeze(-1)    # [batch]\n",
        "                else:\n",
        "                    logits = outputs.squeeze(-1)           # [batch]\n",
        "\n",
        "                preds.extend(logits.cpu().numpy())\n",
        "                trues.extend(labels.cpu().numpy())\n",
        "\n",
        "        fold_metrics.append(compute_metrics(np.array(trues), np.array(preds)))\n",
        "\n",
        "    avg = np.mean(fold_metrics, axis=0)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"MSE\": avg[0],\n",
        "        \"RMSE\": avg[1],\n",
        "        \"MAE\": avg[2],\n",
        "        \"R²\": avg[3],\n",
        "        \"MMRE\": avg[4],\n",
        "        \"Pred(25)\": avg[5]\n",
        "    })\n",
        "\n",
        "# ====================================================\n",
        "# Final comparison table\n",
        "# ====================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== FINAL TRANSFORMER RESULTS (Kitchenham) =====\\n\")\n",
        "print(results_df.sort_values(\"RMSE\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weYCysiexXsC",
        "outputId": "ca4009a1-5211-4112-8c59-4bd47fd85ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running 5-Fold CV for RoBERTa-base...\n",
            "\n",
            "Running 5-Fold CV for GPT-2...\n",
            "\n",
            "===== FINAL TRANSFORMER RESULTS (Kitchenham) =====\n",
            "\n",
            "          Model       MSE      RMSE       MAE        R²      MMRE  Pred(25)\n",
            "0     BERT-base  0.346235  0.484373  0.362807  0.698505  0.389827  0.565517\n",
            "1  RoBERTa-base  0.447225  0.592818  0.476157  0.575531  0.663540  0.365517\n",
            "2         GPT-2  1.397837  1.134409  0.914286 -0.416352  1.021157  0.220690\n"
          ]
        }
      ]
    }
  ]
}