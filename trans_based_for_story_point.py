# -*- coding: utf-8 -*-
"""Trans-based for story point.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iE-Re7C2dOgpSeh-TM09G8EHzj_DIWtL
"""

!pip install drive

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/Software Estimation')

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error

from transformers import BertTokenizer, BertModel
from torch.optim import AdamW


# -------------------------
# Load Dataset
# -------------------------
data = pd.read_csv("data_csv/data")
texts = data["concat"].astype(str).tolist()
labels = data["point"].astype(float).tolist()

train_texts, test_texts, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)


# -------------------------
# Dataset Class
# -------------------------
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

class TextRegDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(),
            "attention_mask": enc["attention_mask"].squeeze(),
            "label": torch.tensor(self.labels[idx], dtype=torch.float)
        }

train_ds = TextRegDataset(train_texts, y_train)
test_ds  = TextRegDataset(test_texts, y_test)

train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=8)


# -------------------------
# BERT Regression Model
# -------------------------
class BertRegressor(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.regressor = torch.nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_emb = outputs.last_hidden_state[:, 0, :]   # CLS token
        return self.regressor(cls_emb).squeeze()

device = "cuda" if torch.cuda.is_available() else "cpu"
model = BertRegressor().to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.MSELoss()


# -------------------------
# Training Loop
# -------------------------
EPOCHS = 10

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attn = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        preds = model(input_ids, attn)
        loss = loss_fn(preds, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS} â€” Loss: {total_loss/len(train_loader):.4f}")


# -------------------------
# Evaluation
# -------------------------
model.eval()
preds_list = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attn = batch["attention_mask"].to(device)

        preds = model(input_ids, attn)
        preds_list.extend(preds.cpu().numpy())

y_pred = np.array(preds_list)
y_test_np = np.array(y_test)

# Metrics
MAE  = mean_absolute_error(y_test_np, y_pred)
MdAE = median_absolute_error(y_test_np, y_pred)
RMSE = np.sqrt(mean_squared_error(y_test_np, y_pred))

# Standardized Accuracy
baseline_pred = np.full_like(y_test_np, np.mean(y_train))
MAE_baseline  = mean_absolute_error(y_test_np, baseline_pred)
SA = 1 - (MAE / MAE_baseline)

# PRED(25)
PRED25 = np.mean((np.abs(y_test_np - y_pred) / y_test_np) <= 0.25)

result_total = {
    "MAE": MAE,
    "MdAE": MdAE,
    "RMSE": RMSE,
    "SA": SA,
    "PRED(25)": PRED25
}

print("\nðŸ“Œ Final BERT Regression Results:")
print(result_total)

import os
os.chdir('/content/drive/MyDrive/Software Estimation')

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error

from transformers import RobertaTokenizer, RobertaModel
from torch.optim import AdamW


# -------------------------
# Load Dataset
# -------------------------
data = pd.read_csv("data_csv/data")
texts = data["concat"].astype(str).tolist()
labels = data["point"].astype(float).tolist()

# Normalize labels for stable training
scaler = StandardScaler()
labels_scaled = scaler.fit_transform(np.array(labels).reshape(-1,1)).flatten()

train_texts, test_texts, y_train, y_test = train_test_split(
    texts, labels_scaled, test_size=0.2, random_state=42
)


# -------------------------
# Dataset Class
# -------------------------
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

class TextRegDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(),
            "attention_mask": enc["attention_mask"].squeeze(),
            "label": torch.tensor(self.labels[idx], dtype=torch.float)
        }

train_ds = TextRegDataset(train_texts, y_train)
test_ds  = TextRegDataset(test_texts, y_test)

train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=8)


# -------------------------
# RoBERTa Regression Model (Mean Pooling)
# -------------------------
class RobertaRegressor(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.roberta = RobertaModel.from_pretrained("roberta-base")
        self.regressor = torch.nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        hidden = outputs.last_hidden_state

        # Mean pooling
        masked = hidden * attention_mask.unsqueeze(-1)
        pooled = masked.sum(1) / attention_mask.sum(1, keepdim=True)

        return self.regressor(pooled).squeeze()


# -------------------------
# Training Setup
# -------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model = RobertaRegressor().to(device)

optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = torch.nn.MSELoss()

EPOCHS = 8

save_path = f"models/SP-reberta.pt"
save_dir = os.path.dirname(save_path)
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
    print(f"ðŸ“ Created directory: {save_dir}")

if os.path.exists(save_path):
    print(f"ðŸ“‚ Found saved model at: {save_path} â€” loading it...")
    checkpoint = torch.load(save_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    print("âœ” Model loaded successfully!")
else:
    print("âš  No saved model found â€” will save after training.")

# -------------------------
# Training Loop
# -------------------------
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attn = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        preds = model(input_ids, attn)
        loss = loss_fn(preds, labels)

        loss.backward()

        # Gradient clipping fixes instability
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS} â€” Loss: {total_loss/len(train_loader):.4f}")
    torch.save({
        "epoch": epoch+1,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict()
    }, save_path)
    print(f"âœ” Model saved: {save_path}")


# -------------------------
# Evaluation
# -------------------------
model.eval()
preds_list = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attn = batch["attention_mask"].to(device)
        preds = model(input_ids, attn)
        preds_list.extend(preds.cpu().numpy())

# Inverse transform predictions
y_pred = scaler.inverse_transform(np.array(preds_list).reshape(-1,1)).flatten()
y_test_orig = scaler.inverse_transform(np.array(y_test).reshape(-1,1)).flatten()


# -------------------------
# Metrics
# -------------------------
MAE  = mean_absolute_error(y_test_orig, y_pred)
MdAE = median_absolute_error(y_test_orig, y_pred)
RMSE = np.sqrt(mean_squared_error(y_test_orig, y_pred))

# Standardized Accuracy
baseline_pred = np.full_like(y_test_orig, np.mean(y_train))
baseline_pred = scaler.inverse_transform(baseline_pred.reshape(-1,1)).flatten()
MAE_baseline  = mean_absolute_error(y_test_orig, baseline_pred)
SA = 1 - (MAE / MAE_baseline)

# PRED(25)
PRED25 = np.mean((np.abs(y_test_orig - y_pred) / y_test_orig) <= 0.25)

results = {
    "MAE": MAE,
    "MdAE": MdAE,
    "RMSE": RMSE,
    "SA": SA,
    "PRED(25)": PRED25
}

print("\nðŸ“Œ Final RoBERTa Regression Results:")
print(results)

import os
os.chdir('/content/drive/MyDrive/Software Estimation')

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error

from transformers import GPT2Tokenizer, GPT2Model
from torch.optim import AdamW

# -------------------------
# Load Dataset
# -------------------------
data = pd.read_csv("data_csv/data")
texts = data["concat"].astype(str).tolist()
labels = data["point"].astype(float).tolist()

train_texts, test_texts, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

# -------------------------
# Dataset Class
# -------------------------
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no PAD token

class TextRegDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(),
            "attention_mask": enc["attention_mask"].squeeze(),
            "label": torch.tensor(self.labels[idx], dtype=torch.float)
        }

train_ds = TextRegDataset(train_texts, y_train)
test_ds  = TextRegDataset(test_texts, y_test)

train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=8)

# -------------------------
# GPT-2 Regression Model
# -------------------------
class GPT2Regressor(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.gpt2 = GPT2Model.from_pretrained("gpt2")
        self.regressor = torch.nn.Linear(self.gpt2.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state  # (batch, seq_len, hidden)
        pooled = last_hidden.mean(dim=1)         # mean pooling
        return self.regressor(pooled).squeeze()

device = "cuda" if torch.cuda.is_available() else "cpu"
model = GPT2Regressor().to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.MSELoss()

# -------------------------
# Training Loop
# -------------------------
EPOCHS = 10
save_path = f"models/SP-gpt2.pt"


if os.path.exists(save_path):
    print(f"ðŸ“‚ Found saved model at: {save_path} â€” loading it...")
    checkpoint = torch.load(save_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    print("âœ” Model loaded successfully!")
else:
    print("âš  No saved model found â€” will save after training.")

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attn = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        preds = model(input_ids, attn)
        loss = loss_fn(preds, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS} â€” Loss: {total_loss/len(train_loader):.4f}")

    torch.save({
        "epoch": epoch+1,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict()
    }, save_path)
    print(f"âœ” Model saved: {save_path}")

# -------------------------
# Evaluation
# -------------------------
model.eval()
preds_list = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attn = batch["attention_mask"].to(device)

        preds = model(input_ids, attn)
        preds_list.extend(preds.cpu().numpy())

y_pred = np.array(preds_list)
y_test_np = np.array(y_test)

# Metrics
MAE  = mean_absolute_error(y_test_np, y_pred)
MdAE = median_absolute_error(y_test_np, y_pred)
RMSE = np.sqrt(mean_squared_error(y_test_np, y_pred))

# Standardized Accuracy
baseline_pred = np.full_like(y_test_np, np.mean(y_train))
MAE_baseline  = mean_absolute_error(y_test_np, baseline_pred)
SA = 1 - (MAE / MAE_baseline)

# PRED(25)
PRED25 = np.mean((np.abs(y_test_np - y_pred) / y_test_np) <= 0.25)

result_total = {
    "MAE": MAE,
    "MdAE": MdAE,
    "RMSE": RMSE,
    "SA": SA,
    "PRED(25)": PRED25
}

print("\nðŸ“Œ Final GPT-2 Regression Results:")
print(result_total)