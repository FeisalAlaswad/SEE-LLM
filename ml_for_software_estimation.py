# -*- coding: utf-8 -*-
"""ML for software estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1isQRUutjwRO68TdYXB0Z2lDZOxfoeh2l
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import KFold

# ----------------------------------------------------
# Load Dataset
# ----------------------------------------------------
projects = pd.read_csv("desharnais.csv")

# Fix column names
projects = projects.rename(columns={'PointsAjust': 'PointsAdjust'})

# Drop unused columns
projects.drop(['id', 'Project','YearEnd','PointsNonAdjust','Adjustment'], axis=1, inplace=True)

# ----------------------------------------------------
# Log Transformation
# ----------------------------------------------------
cols_to_log_transform = [
    'Effort',
    'Length',
    'Transactions',
    'Entities',
    'PointsAdjust'   # already renamed
]


projects[cols_to_log_transform] = np.log(projects[cols_to_log_transform])

# ----------------------------------------------------
# Normalization of some features
# ----------------------------------------------------
cols_to_normalize = ['TeamExp', 'ManagerExp', 'Language']
projects[cols_to_normalize] = StandardScaler().fit_transform(projects[cols_to_normalize])

# ----------------------------------------------------
# Features and Target
# ----------------------------------------------------
X = projects.drop('Effort', axis=1).values
y = projects['Effort'].values

# ----------------------------------------------------
# Define Models
# ----------------------------------------------------
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "SVR (RBF Kernel)": SVR(kernel='rbf', C=100, gamma='scale'),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
}

# Add XGBoost if installed
try:
    from xgboost import XGBRegressor
    models["XGBoost"] = XGBRegressor(
        n_estimators=300, learning_rate=0.05, max_depth=4,
        subsample=0.9, random_state=42, eval_metric='rmse'
    )
except:
    print("XGBoost not installed — skipping.")

# ----------------------------------------------------
# Function to compute metrics
# ----------------------------------------------------
def compute_metrics(y_true_log, y_pred_log):
    y_true = np.exp(y_true_log)
    y_pred = np.exp(y_pred_log)

    mse = mean_squared_error(y_true_log, y_pred_log)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true_log, y_pred_log)
    r2 = r2_score(y_true_log, y_pred_log)

    mre = np.abs((y_true - y_pred) / y_true)
    mmre = np.mean(mre)
    pred_25 = np.mean(mre <= 0.25)

    return mse, rmse, mae, r2, mmre, pred_25

# ----------------------------------------------------
# 5-Fold Cross-Validation
# ----------------------------------------------------
kf = KFold(n_splits=5, shuffle=True, random_state=42)
results = []

for name, model in models.items():
    print(f"Running 5-fold CV for {name}...")
    fold_metrics = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        fold_metrics.append(compute_metrics(y_test, y_pred))

    # Average metrics over folds
    avg_metrics = np.mean(fold_metrics, axis=0)
    results.append({
        "Model": name,
        "MSE": avg_metrics[0],
        "RMSE": avg_metrics[1],
        "MAE": avg_metrics[2],
        "R²": avg_metrics[3],
        "MMRE": avg_metrics[4],
        "Pred(25)": avg_metrics[5]
    })

# ----------------------------------------------------
# Display Results
# ----------------------------------------------------
results_df = pd.DataFrame(results)
print("\n===== FINAL COMPARISON TABLE (5-Fold CV) =====\n")
print(results_df)
print("\n===== SORTED BY RMSE =====\n")
print(results_df.sort_values("RMSE"))

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load Dataset
cocomo = pd.read_csv("COCOMO-81.csv")

# Encode 'dev_mode' (embedded, semidetached, organic)
cocomo["dev_mode"] = cocomo["dev_mode"].astype("category").cat.codes

# Log-transform skewed columns
cols_to_log_transform = ["actual", "loc"]
cocomo[cols_to_log_transform] = np.log(cocomo[cols_to_log_transform])

#cols_to_drop = ["lexp", "vexp", "turn", "virt"]
#cocomo = cocomo.drop(columns=cols_to_drop)

"""
cols_to_normalize = [
    'rely','data','cplx','time','stor',
    'acap','aexp','pcap','modp','tool','sced',
    'dev_mode'
]
"""
# Normalize numerical features
cols_to_normalize = [
    'rely','data','cplx','time','stor','virt','turn',
    'acap','aexp','pcap','vexp','lexp','modp','tool','sced',
    'dev_mode'
]
cocomo[cols_to_normalize] = StandardScaler().fit_transform(cocomo[cols_to_normalize])

# Features and target
X = cocomo.drop("actual", axis=1).values

y = cocomo["actual"].values

# Define models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "SVR (RBF Kernel)": SVR(kernel='rbf', C=150, gamma='scale'),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
}

try:
    from xgboost import XGBRegressor
    models["XGBoost"] = XGBRegressor(
        n_estimators=300, learning_rate=0.05, max_depth=4, subsample=0.9, random_state=42, eval_metric='rmse'
    )
except:
    print("XGBoost not installed — skipping.")

# Evaluation function
def evaluate_fold(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Reverse log-transform for original-scale metrics
    y_test_real = np.exp(y_test)
    y_pred_real = np.exp(y_pred)

    # Metrics (log-scale)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # MRE & MMRE
    mre = np.abs((y_test_real - y_pred_real) / y_test_real)
    mmre = np.mean(mre)
    pred_25 = np.mean(mre <= 0.25)

    return mse, rmse, mae, r2, mmre, pred_25

# 5-Fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

results = []

for name, model in models.items():
    mse_list, rmse_list, mae_list, r2_list, mmre_list, pred25_list = [], [], [], [], [], []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        mse, rmse, mae, r2, mmre, pred25 = evaluate_fold(model, X_train, X_test, y_train, y_test)
        mse_list.append(mse)
        rmse_list.append(rmse)
        mae_list.append(mae)
        r2_list.append(r2)
        mmre_list.append(mmre)
        pred25_list.append(pred25)

    # Average metrics across folds
    results.append({
        "Model": name,
        "MSE": np.mean(mse_list),
        "RMSE": np.mean(rmse_list),
        "MAE": np.mean(mae_list),
        "R²": np.mean(r2_list),
        "MMRE": np.mean(mmre_list),
        "Pred(25)": np.mean(pred25_list)
    })

# Display results
results_df = pd.DataFrame(results)
print("\n===== 5-Fold CV RESULTS =====\n")
print(results_df.sort_values("RMSE"))

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ----------------------------------------------------
# Load NASA93 Dataset
# ----------------------------------------------------
nasa = pd.read_csv("NASA_93_Sheet.csv")
print(nasa.head())

# ----------------------------------------------------
# Select ONLY needed features
# ----------------------------------------------------
selected_features = [
    "prec","flex","resl","team","pmat","rely","data","cplx","ruse","docu",
    "time","stor","pvol","acap","pcap","pcon","apex","plex","ltex","tool",
    "site","sced","kloc"
]

# Ensure dataset contains them
missing = [f for f in selected_features if f not in nasa.columns]
if missing:
    print("❌ Missing columns:", missing)
else:
    print("✔ All selected columns found.")

# ----------------------------------------------------
# Define X and y
# ----------------------------------------------------
X = nasa[selected_features].values
y = nasa["effort"].values          # target variable

# Optional: log-transform target
y_log = np.log(y)

# ----------------------------------------------------
# Normalize predictors
# ----------------------------------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ----------------------------------------------------
# Define Models
# ----------------------------------------------------
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "SVR (RBF Kernel)": SVR(kernel='rbf', C=100, gamma='scale'),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
}

# Try XGBoost
try:
    from xgboost import XGBRegressor
    models["XGBoost"] = XGBRegressor(
        n_estimators=300, learning_rate=0.05, max_depth=4,
        subsample=0.9, random_state=42, eval_metric='rmse'
    )
except:
    print("XGBoost not installed — skipping.")

# ----------------------------------------------------
# Evaluation Function
# ----------------------------------------------------
def evaluate_fold(y_true, y_pred):
    y_true_real = np.exp(y_true)
    y_pred_real = np.exp(y_pred)

    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    mre = np.abs((y_true_real - y_pred_real) / y_true_real)
    mmre = np.mean(mre)
    pred_25 = np.mean(mre <= 0.25)

    return mse, rmse, mae, r2, mmre, pred_25

# ----------------------------------------------------
# 5-Fold Cross-Validation
# ----------------------------------------------------
kf = KFold(n_splits=5, shuffle=True, random_state=42)
results = []

for name, model in models.items():
    print(f"Running 5-Fold CV for {name}...")
    metrics = {"MSE": [], "RMSE": [], "MAE": [], "R²": [], "MMRE": [], "Pred(25)": []}

    for train_idx, test_idx in kf.split(X_scaled):
        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
        y_train, y_test = y_log[train_idx], y_log[test_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        mse, rmse, mae, r2, mmre, pred_25 = evaluate_fold(y_test, y_pred)

        metrics["MSE"].append(mse)
        metrics["RMSE"].append(rmse)
        metrics["MAE"].append(mae)
        metrics["R²"].append(r2)
        metrics["MMRE"].append(mmre)
        metrics["Pred(25)"].append(pred_25)

    results.append({
        "Model": name,
        "MSE": np.mean(metrics["MSE"]),
        "RMSE": np.mean(metrics["RMSE"]),
        "MAE": np.mean(metrics["MAE"]),
        "R²": np.mean(metrics["R²"]),
        "MMRE": np.mean(metrics["MMRE"]),
        "Pred(25)": np.mean(metrics["Pred(25)"])
    })

# ----------------------------------------------------
# Display Results
# ----------------------------------------------------
results_df = pd.DataFrame(results)
print("\n===== FINAL 5-FOLD CV RESULTS =====\n")
print(results_df.sort_values("RMSE"))

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ----------------------------------------------------
# Load Dataset
# ----------------------------------------------------
projects = pd.read_csv("maxwell.csv")
if 'Syear' in projects.columns:
    projects.drop(['Syear'], axis=1, inplace=True)

target = 'Effort'
feature_cols = [c for c in projects.columns if c != target]

X = projects[feature_cols]
y = projects[target]

# ----------------------------------------------------
# Log-transform skewed continuous variables
# ----------------------------------------------------
cols_to_log = ['Effort', 'Duration', 'Size', 'Time']
for col in cols_to_log:
    if col in projects.columns:
        projects[col] = np.log(projects[col].replace(0, 1))  # avoid log(0)

# ----------------------------------------------------
# Normalize categorical-like integers
# ----------------------------------------------------
categorical_like = [
    'App','Har','Dba','Ifc','Source','Telonuse','Nlan'
] + [f"T{i:02d}" for i in range(1,16) if f"T{i:02d}" in projects.columns]

projects[categorical_like] = StandardScaler().fit_transform(projects[categorical_like])

X = projects.drop('Effort', axis=1)
y = projects['Effort']

# ----------------------------------------------------
# Define Models
# ----------------------------------------------------
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "SVR (RBF Kernel)": SVR(kernel='rbf', C=100, gamma='scale'),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
}

try:
    from xgboost import XGBRegressor
    models["XGBoost"] = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4,
                                     subsample=0.9, random_state=42, eval_metric='rmse')
except:
    print("XGBoost not installed — skipping.")

# ----------------------------------------------------
# K-Fold Cross-Validation
# ----------------------------------------------------
kf = KFold(n_splits=5, shuffle=True, random_state=42)

def evaluate_model_cv(name, model, X, y, kf):
    mse_list, rmse_list, mae_list, r2_list, mmre_list, pred25_list = [], [], [], [], [], []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Reverse log-transform
        y_test_real = np.exp(y_test)
        y_pred_real = np.exp(y_pred)

        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        mre = np.abs(y_test_real - y_pred_real) / y_test_real
        mmre = np.mean(mre)
        pred_25 = np.mean(mre <= 0.25)

        mse_list.append(mse)
        rmse_list.append(rmse)
        mae_list.append(mae)
        r2_list.append(r2)
        mmre_list.append(mmre)
        pred25_list.append(pred_25)

    return {
        "Model": name,
        "MSE": np.mean(mse_list),
        "RMSE": np.mean(rmse_list),
        "MAE": np.mean(mae_list),
        "R²": np.mean(r2_list),
        "MMRE": np.mean(mmre_list),
        "Pred(25)": np.mean(pred25_list)
    }

# ----------------------------------------------------
# Evaluate all models
# ----------------------------------------------------
results = []
for name, model in models.items():
    print(f"Running 5-fold CV for {name}...")
    results.append(evaluate_model_cv(name, model, X, y, kf))

results_df = pd.DataFrame(results)
print("\n===== 5-FOLD CV RESULTS =====\n")
print(results_df.sort_values("RMSE"))

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ----------------------------------------------------
# Load Dataset
# ----------------------------------------------------
projects = pd.read_csv("kitchenham.csv")

# Drop irrelevant columns
drop_cols = [
    'id', 'Project', 'Client.code',
    'Actual.start.date', 'Estimated.completion.date',
    'First.estimate.method'
]
projects.drop(columns=[c for c in drop_cols if c in projects.columns], inplace=True)

target = 'Actual.effort'

# ----------------------------------------------------
# Encode categorical STRING columns
# ----------------------------------------------------
cat_cols = projects.select_dtypes(include=['object']).columns

label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    projects[col] = le.fit_transform(projects[col])
    label_encoders[col] = le

# ----------------------------------------------------
# Log-transform skewed numeric columns
# ----------------------------------------------------
cols_to_log = ['Actual.effort', 'Actual.duration', 'Adjusted.function.points', 'First.estimate']
for col in cols_to_log:
    if col in projects.columns:
        projects[col] = np.log(projects[col].replace(0, 1))

# ----------------------------------------------------
# Scale numeric features
# ----------------------------------------------------
num_cols = [c for c in projects.columns if c != target]
scaler = StandardScaler()
projects[num_cols] = scaler.fit_transform(projects[num_cols])

X = projects.drop(target, axis=1)
y = projects[target]

# ----------------------------------------------------
# Define Models
# ----------------------------------------------------
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "SVR (RBF Kernel)": SVR(kernel='rbf', C=100, gamma='scale'),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
}

try:
    from xgboost import XGBRegressor
    models["XGBoost"] = XGBRegressor(
        n_estimators=300, learning_rate=0.05,
        max_depth=4, subsample=0.9,
        random_state=42, eval_metric='rmse'
    )
except:
    print("XGBoost not installed — skipping.")

# ----------------------------------------------------
# K-Fold Evaluation
# ----------------------------------------------------
kf = KFold(n_splits=5, shuffle=True, random_state=42)

def evaluate_model_cv(name, model, X, y, kf):
    mse_list, rmse_list, mae_list, r2_list, mmre_list, pred25_list = [], [], [], [], [], []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Reverse log
        y_test_real = np.exp(y_test)
        y_pred_real = np.exp(y_pred)

        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        mre = np.abs(y_test_real - y_pred_real) / y_test_real
        mmre = np.mean(mre)
        pred_25 = np.mean(mre <= 0.25)

        mse_list.append(mse)
        rmse_list.append(rmse)
        mae_list.append(mae)
        r2_list.append(r2)
        mmre_list.append(mmre)
        pred25_list.append(pred_25)

    return {
        "Model": name,
        "MSE": np.mean(mse_list),
        "RMSE": np.mean(rmse_list),
        "MAE": np.mean(mae_list),
        "R²": np.mean(r2_list),
        "MMRE": np.mean(mmre_list),
        "Pred(25)": np.mean(pred25_list)
    }

# ----------------------------------------------------
# Run All Models
# ----------------------------------------------------
results = []
for name, model in models.items():
    print(f"Running 5-fold CV for {name}...")
    results.append(evaluate_model_cv(name, model, X, y, kf))

results_df = pd.DataFrame(results)
print("\n===== 5-FOLD CV RESULTS =====\n")
print(results_df.sort_values("RMSE"))

import pandas as pd

# ------------------------------------------------
# INSERT YOUR RESULTS HERE
# ------------------------------------------------
results_dict = {
    # ----- ML FAMILY -----
    "SVR": {
        "Family": "ML",
        "MAE": 0.4784, "RMSE": 0.6221,
        "PRED(25)": 0.3607
    },
    "RandomForest": {
        "Family": "ML",
        "MAE": 0.4521, "RMSE": 0.5742,
        "PRED(25)": 0.3997
    },
    "XGBoost": {
        "Family": "ML",
        "MAE": 0.4778, "RMSE": 0.6014,
        "PRED(25)": 0.3615
    },
    "LinearReg": {
        "Family": "ML",
        "MAE": 0.4489, "RMSE": 0.5780,
        "PRED(25)": 0.4306
    },
    "KNN": {
        "Family": "ML",
        "MAE": 0.6772, "RMSE": 0.8695,
        "PRED(25)": 0.2982
    },

    # ----- TRANSFORMER FAMILY -----
    "BERT": {
        "Family": "Transformer",
        "MAE": 1.5038, "RMSE": 1.6929,
        "PRED(25)": 0.2201
    },
    "RoBERTa": {
        "Family": "Transformer",
        "MAE": 1.0111, "RMSE": 1.2093,
        "PRED(25)": 0.2049
    },
    "GPT2": {
        "Family": "Transformer",
        "MAE": 1.7019, "RMSE": 1.9387,
        "PRED(25)": 0.1105
    },

    # ----- LLM FAMILY -----
    "GPT-4-Turbo": {
        "Family": "LLM",
        "MAE": 1.2112, "RMSE": 1.4380,
        "PRED(25)": 0.2044
    },
    "DeepSeek-V3": {
        "Family": "LLM",
        "MAE": 1.3318, "RMSE": 1.5638,
        "PRED(25)": 0.1736
    },
    "Llama-3-70B": {
        "Family": "LLM",
        "MAE": 1.5218, "RMSE": 1.7714,
        "PRED(25)": 0.1134
    },
    "Gemini-1.5-Pro": {
        "Family": "LLM",
        "MAE": 1.2218, "RMSE": 1.4486,
        "PRED(25)": 0.2042
    }
}


# Convert dict → DataFrame
df = pd.DataFrame.from_dict(results_dict, orient='index').reset_index()

# Rename first column only
df = df.rename(columns={"index": "Model"})
df = df.rename(columns={"PRED(25)": "PRED25"})

print(df)

df["Family"] = df["Family"].astype("category")
df["Family"] = df["Family"].cat.reorder_categories(["ML", "Transformer", "LLM"], ordered=True)

import statsmodels.formula.api as smf

model_lme = smf.mixedlm(
    "MAE ~ Family",
    data=df,
    groups=df["Model"]       # random intercept per model
)

lme_result = model_lme.fit()
print(lme_result.summary())


beta_trans = lme_result.params.get("Family[T.Transformer]")
beta_llm = lme_result.params.get("Family[T.LLM]")
mu = lme_result.params.get("Intercept")

pval_trans = lme_result.pvalues.get("Family[T.Transformer]")
pval_llm = lme_result.pvalues.get("Family[T.LLM]")

print("\n--- Interpretation ---")
print(f"Baseline (ML family) MAE = {mu:.3f}")

print(f"Transformer effect (β_Trans) = {beta_trans:.3f}  | p = {pval_trans:.5f}")
print(f"LLM effect (β_LLM) = {beta_llm:.3f}          | p = {pval_llm:.5f}")

# Significance interpretation
if pval_trans < 0.001:
    print("✔ Transformer family differs significantly from ML (p < 0.001)")

if pval_llm < 0.001:
    print("✔ LLM family differs significantly from ML (p < 0.001)")